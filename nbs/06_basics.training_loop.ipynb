{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp basics.training_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the fastai Training Loop Further\n",
    "> Extending fastai's `show_training_loop` to be more verbose about event triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from wwf.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "---\n",
       "This article is also a Jupyter Notebook available to be run from the top down. There\n",
       "will be code snippets that you can then run in any environment.\n",
       "\n",
       "Below are the versions of `fastai`, `fastcore`, and `wwf` currently running at the time of writing this:\n",
       "* `fastai`: 2.2.5 \n",
       "* `fastcore`: 1.3.19 \n",
       "* `wwf`: 0.0.10 \n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "state_versions(['fastai', 'fastcore', 'wwf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding fastai's Training Loop\n",
    "\n",
    "`fastai`'s training loop is certainly unique in its approach, where everything is handled through `Callbacks`. What this means is there should *never* be an instance where if you need to modify `fastai`'s training loop you are modifying `Learner`'s source code. \n",
    "\n",
    "Instead we can use various trigger points through `Callbacks` to get there. Currently fastai has a methodology of showing just what `Callbacks` are called during the training loop through a function called `Learner.show_training_loop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `show_training_loop`\n",
    "\n",
    "The goal of `show_training_loop` is to show the user just what `Callbacks` are triggered during fastai's entire training cycle. An example is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fit\n",
      "   - before_fit     : [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "  Start Epoch Loop\n",
      "     - before_epoch   : [Recorder, ProgressCallback]\n",
      "    Start Train\n",
      "       - before_train   : [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      Start Batch Loop\n",
      "         - before_batch   : []\n",
      "         - after_pred     : []\n",
      "         - after_loss     : []\n",
      "         - before_backward: []\n",
      "         - before_step    : []\n",
      "         - after_step     : []\n",
      "         - after_cancel_batch: []\n",
      "         - after_batch    : [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      End Batch Loop\n",
      "    End Train\n",
      "     - after_cancel_train: [Recorder]\n",
      "     - after_train    : [Recorder, ProgressCallback]\n",
      "    Start Valid\n",
      "       - before_validate: [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      Start Batch Loop\n",
      "         - **CBs same as train batch**: []\n",
      "      End Batch Loop\n",
      "    End Valid\n",
      "     - after_cancel_validate: [Recorder]\n",
      "     - after_validate : [Recorder, ProgressCallback]\n",
      "  End Epoch Loop\n",
      "   - after_cancel_epoch: []\n",
      "   - after_epoch    : [Recorder]\n",
      "End Fit\n",
      " - after_cancel_fit: []\n",
      " - after_fit      : [ProgressCallback]\n"
     ]
    }
   ],
   "source": [
    "from fastai.callback.all import *\n",
    "from fastai.test_utils import synth_learner\n",
    "\n",
    "learn = synth_learner()\n",
    "learn.show_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, every major event is detailed with a `Start` and `Finish`, and the intermediate steps at each level are described. Paired with this are the `Callbacks` that get triggered at that particular event.\n",
    "\n",
    "However, I think we can take this a step further to enable you to understand *just* what happens during each step. As a result, I've written a revised version of `Learner.show_training_loop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from fastcore.xtras import dict2obj\n",
    "\n",
    "_event2doc = {\n",
    "    'after_create': \"Called after the `Learner` is created\",\n",
    "    'before_fit': \"Called before starting training or inference, ideal for initial setup\",\n",
    "    'before_epoch': \"Called at the beginning of each epoch, useful for any behavior you need to reset at each epoch\",\n",
    "    'before_train': \"Called at the beginning of the training part of an epoch\",\n",
    "    'before_batch': \"Called at the beginning of each batch, just after drawing said batch.\\nIt can be used to do any setup necessary for the batch or to change the input/target before it goes in the model\",\n",
    "    'after_pred': \"Called after computing the output of the model on the batch. It can be used to change that output before it's fed to the loss\",\n",
    "    'after_loss': \"Called after the loss has been computed, but before the backward pass. It can be used to add any penalty to the loss\",\n",
    "    'before_backward': \"Called after the loss has been computed, but only in training mode (i.e. when the backward pass will be used)\",\n",
    "    'before_step': \"Called after the backward pass, but before the update of the parameters. It can be used to do any change to the gradients before said update\",\n",
    "    'after_step': \"Called after the step and before the gradients are zeroed\",\n",
    "    'after_batch': \"Called at the end of a batch, for any clean-up before the next one\",\n",
    "    'after_train': \"Called at the end of the training phase of an epoch\",\n",
    "    'before_validate': \"Called at the beginning of the validation phase of an epoch, useful for any setup needed specifically for validation\",\n",
    "    'after_validate': \"Called at the end of the validation part of an epoch\",\n",
    "    'after_epoch': \"Called at the end of an epoch, for any clean-up before the next one\",\n",
    "    'after_fit': \"Called at the end of training, for final clean-up\",\n",
    "    'after_cancel_batch': \"Reached immediately after a CancelBatchException before proceeding to after_batch\",\n",
    "    'after_cancel_train': \"Reached immediately after a CancelTrainException before proceeding to after_epoch\",\n",
    "    'after_cancel_validate': \"Reached immediately after a CancelValidException before proceeding to after_epoch\",\n",
    "    'after_cancel_epoch': \"Reached immediately after a CancelEpochException before proceeding to after_epoch\",\n",
    "    'after_cancel_fit': \"Reached immediately after a CancelFitException before proceeding to after_fit\"\n",
    "}\n",
    "\n",
    "event2doc = dict2obj(_event2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Union, List\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastcore.dispatch import patch\n",
    "from fastcore.xtras import is_listy, listify\n",
    "from fastai.learner import _loop, Learner #list of all fastai events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _print_cb(cb:Callback, event:str, indent:int=0):\n",
    "    \"Prints what `cb` does during `event` with potential `indent`\"\n",
    "    if getattr(cb, event).__doc__ is not None:\n",
    "        print(f'{\" \"*(indent+4)} - {cb}: \\n{\" \"*(indent+8)} - {getattr(cb, event).__doc__}')\n",
    "    else:\n",
    "        print(f'{\" \"*(indent+4)} - {cb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def show_training_loop(self:Learner, verbose:bool=False, cbs:Union[None,list,Callback]=None):\n",
    "    \"Show each step in the training loop, potentially with Callback event descriptions\"\n",
    "    if cbs is not None: self.add_cbs(cbs) if is_listy(cbs) else self.add_cbs(listify(cbs))\n",
    "    indent = 0\n",
    "    for s in _loop:\n",
    "        if s.startswith('Start'): print(f'{\" \"*indent}{s}'); indent += 3\n",
    "        elif s.startswith('End'): indent -= 3; print(f'{\" \"*indent}{s}')\n",
    "        else:\n",
    "            if not verbose:\n",
    "                print(f'{\" \"*indent} - {s}:', self.ordered_cbs(s))\n",
    "            else:\n",
    "                print(f'{\" \"*indent} - {s}:')\n",
    "                for cb in self.ordered_cbs(s): \n",
    "                    _print_cb(cb, s, indent)\n",
    "    if cbs is not None: self.remove_cbs(cbs) if is_listy(cbs) else self.remove_cbs(listify(cbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new version we can pass in a `verbose` tag and for every `Callback` and its events we will pull its documentation string, so we can see what happens at each step as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fit\n",
      "    - before_fit:\n",
      "        - TrainEvalCallback: \n",
      "            - Set the iter and epoch counters to 0, put the model and the right device\n",
      "        - Recorder: \n",
      "            - Prepare state for training\n",
      "        - ProgressCallback: \n",
      "            - Setup the master bar over the epochs\n",
      "   Start Epoch Loop\n",
      "       - before_epoch:\n",
      "           - Recorder: \n",
      "               - Set timer if `self.add_time=True`\n",
      "           - ProgressCallback: \n",
      "               - Update the master bar\n",
      "      Start Train\n",
      "          - before_train:\n",
      "              - TrainEvalCallback: \n",
      "                  - Set the model in training mode\n",
      "              - Recorder: \n",
      "                  - Reset loss and metrics state\n",
      "              - ProgressCallback: \n",
      "                  - Launch a progress bar over the training dataloader\n",
      "         Start Batch Loop\n",
      "             - before_batch:\n",
      "             - after_pred:\n",
      "             - after_loss:\n",
      "             - before_backward:\n",
      "             - before_step:\n",
      "             - after_step:\n",
      "             - after_cancel_batch:\n",
      "             - after_batch:\n",
      "                 - TrainEvalCallback: \n",
      "                     - Update the iter counter (in training mode)\n",
      "                 - Recorder: \n",
      "                     - Update all metrics and records lr and smooth loss in training\n",
      "                 - ProgressCallback: \n",
      "                     - Update the current progress bar\n",
      "         End Batch Loop\n",
      "      End Train\n",
      "       - after_cancel_train:\n",
      "           - Recorder: \n",
      "               - Ignore training metrics for this epoch\n",
      "       - after_train:\n",
      "           - Recorder: \n",
      "               - Log loss and metric values on the training set (if `self.training_metrics=True`)\n",
      "           - ProgressCallback: \n",
      "               - Close the progress bar over the training dataloader\n",
      "      Start Valid\n",
      "          - before_validate:\n",
      "              - TrainEvalCallback: \n",
      "                  - Set the model in validation mode\n",
      "              - Recorder: \n",
      "                  - Reset loss and metrics state\n",
      "              - ProgressCallback: \n",
      "                  - Launch a progress bar over the validation dataloader\n",
      "         Start Batch Loop\n",
      "             - **CBs same as train batch**:\n",
      "         End Batch Loop\n",
      "      End Valid\n",
      "       - after_cancel_validate:\n",
      "           - Recorder: \n",
      "               - Ignore validation metrics for this epoch\n",
      "       - after_validate:\n",
      "           - Recorder: \n",
      "               - Log loss and metric values on the validation set\n",
      "           - ProgressCallback: \n",
      "               - Close the progress bar over the validation dataloader\n",
      "   End Epoch Loop\n",
      "    - after_cancel_epoch:\n",
      "    - after_epoch:\n",
      "        - Recorder: \n",
      "            - Store and log the loss/metric values\n",
      "End Fit\n",
      " - after_cancel_fit:\n",
      " - after_fit:\n",
      "     - ProgressCallback: \n",
      "         - Close the master bar\n"
     ]
    }
   ],
   "source": [
    "learn.show_training_loop(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example:\n",
    "\n",
    "To use this functionality, simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwf.basics.training_loop import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then call `learn.show_training_loop(verbose=True)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
